@inproceedings{Gururangan2018Annotation,
	author = {Gururangan, Suchin and Swayamdipta, Swabha and Levy, Omer and Schwartz, Roy and Bowman, Samuel and Smith, Noah A.},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of
          the {Association} for {Computational} {Linguistics}: Human {Language}
          {Technologies}, {Volume} 2 ({Short} {Papers})},
	year = {2018},
	organization = {Association for Computational Linguistics},
	title = {Annotation {Artifacts} in {Natural} {Language} {Inference} {Data}},
	url = {http://dx.doi.org/10.18653/v1/N18-2017},
	doi = {10.18653/v1/n18-2017},
	abstract = {Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67% of SNLI (Bowman et. al, 2015) and 53% of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.},
}


@inproceedings{Alnegheimish2022Using,
	author = {Alnegheimish, Sarah and Guo, Alicia and Sun, Yi},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: Human {Language} {Technologies}},
	year = {2022},
	organization = {Association for Computational Linguistics},
	title = {Using {Natural} {Sentence} {Prompts} for {Understanding} {Biases} in {Language} {Models}},
	url = {http://dx.doi.org/10.18653/v1/2022.naacl-main.203},
	doi = {10.18653/v1/2022.naacl-main.203},
	abstract = {Evaluation of biases in language models is often limited to synthetically generated datasets. This dependence traces back to the need of prompt-style dataset to trigger specific behaviors of language models. In this paper, we address this gap by creating a prompt dataset with respect to occupations collected from real-world natural sentences present in Wikipedia.We aim to understand the differences between using template-based prompts and natural sentence prompts when studying gender-occupation biases in language models. We find bias evaluations are very sensitiveto the design choices of template prompts, and we propose using natural sentence prompts as a way of more systematically using real-world sentences to move away from design decisions that may bias the results.},
}


@misc{PaulTowards,
	author = {{Paul Pu Liang} and {Chiyu Wu} and {Louis-Philippe Morency} and {R. Salakhutdinov}},
	title = {Towards {Understanding} and {Mitigating} {Social} {Biases} in {Language} {Models}},
	abstract = {Warning: this paper contains model outputs that may be offensive or upsetting. As machine learning methods are deployed in realworld settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for highfidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.},
}


@article{Alnegheimish2022Using,
	author = {Alnegheimish, Sarah and Guo, Alicia and Sun, Yi},
	year = {2022},
	publisher = {arXiv},
	title = {Using {Natural} {Sentences} for {Understanding} {Biases} in {Language} {Models}},
	url = {https://arxiv.org/abs/2205.06303},
	doi = {10.48550/ARXIV.2205.06303},
	abstract = {Evaluation of biases in language models is often limited to synthetically generated datasets. This dependence traces back to the need of prompt-style dataset to trigger specific behaviors of language models. In this paper, we address this gap by creating a prompt dataset with respect to occupations collected from real-world natural sentences present in Wikipedia. We aim to understand the differ-ences between using template-based prompts and natural sentence prompts when studying gender-occupation biases in language models. We find bias evaluations are very sensitive to the design choices of template prompts, and we propose using natural sentence prompts for systematic evaluations to step away from design choices that could introduce bias in the observations.},
}


@inproceedings{Chen2020General,
	author = {Chen, Jiawei and Xu, Anbang and Liu, Zhe and Guo, Yufan and Liu, Xiaotong and Tong, Yingbei and Akkiraju, Rama and Carroll, John M.},
	booktitle = {Extended {Abstracts} of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	year = {2020},
	month = {apr 25},
	organization = {ACM},
	title = {A {General} {Methodology} to {Quantify} {Biases} in {Natural} {Language} {Data}},
	url = {http://dx.doi.org/10.1145/3334480.3382949},
	doi = {10.1145/3334480.3382949},
	abstract = {Biases in data, such as gender and racial stereotypes, are propagated through intelligent systems and amplified at end-user applications. Existing studies detect and quantify biases based on pre-defined attributes. However, in real practices, it is difficult to gather a comprehensive list of sensitive concepts for various categories of biases. We propose a general methodology to quantify dataset biases by measuring the difference of its data distribution with a reference dataset using Maximum Mean Discrepancy. For the case of natural language data, we show that lexicon-based features quantify explicit stereotypes, while deep learning-based features further capture implicit stereotypes represented by complex semantics. Our method provides a more flexible way to detect potential biases.},
}


@misc{ShanshanStatistically,
	author = {{Shanshan Huang} and {Kenny Q. Zhu}},
	title = {Statistically {Profiling} {Biases} in {Natural} {Language} {Reasoning} {Datasets} and {Models}},
	abstract = {Recent work has indicated that many natural language understanding and reasoning datasets contain statistical cues that may be taken advantaged of by NLP models whose capability may thus be grossly overestimated. To discover the potential weakness in the models, some human-designed stress tests have been proposed but they are expensive to create and do not generalize to arbitrary models. We propose a light-weight and general statistical profiling framework, ICQ (I-See-Cue), which automatically identifies possible biases in any multiple-choice NLU datasets without the need to create any additional test cases, and further evaluates through blackbox testing the extent to which models may exploit these biases.},
}


@misc{EzgiDisclosing,
	author = {{Ezgi Korkmaz}},
	title = {Disclosing the {Biases} in {Large} {Language} {Models} via {Reward} {Based} {Interrogation}},
	abstract = {The success of large language models has been utterly demonstrated in recent times. Using these models and fine tuning for the specific task at hand results in high performance. However, these models also learn biased representations from the data they have been trained on. In particular, several studies recently showed that language models can learn to be biased towards certain genders. Quite recently, several studies tried to eliminate this bias via proposing human feedback included in fine-tuning. In our study we show that by changing the question asked to the language model the log probabilities of the bias measured in the responses changes dramatically. Furthermore, in several cases the language model ends up providing a completely opposite response. The recent language models finetuned on the prior gender bias datasets do not resolve the actual problem, but rather alleviate the problem for the dataset on which the model is fine-tuned. We believe our results might lay the foundation for further alignment and safety problems in large language models.},
}


@inproceedings{Chaabouni2019Word,
	author = {Chaabouni, Rahma and Kharitonov, Eugene and Lazaric, Alessandro and Dupoux, Emmanuel and Baroni, Marco},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	year = {2019},
	organization = {Association for Computational Linguistics},
	title = {Word-order {Biases} in {Deep}-agent {Emergent} {Communication}},
	url = {http://dx.doi.org/10.18653/v1/P19-1509},
	doi = {10.18653/v1/p19-1509},
	abstract = {Sequence-processing neural networks led to remarkable progress on many NLP tasks. As a consequence, there has been increasing interest in understanding to what extent they process language as humans do. We aim here to uncover which biases such models display with respect to ``natural'' word-order constraints. We train models to communicate about paths in a simple gridworld, using miniature languages that reflect or violate various natural language trends, such as the tendency to avoid redundancy or to minimize long-distance dependencies. We study how the controlled characteristics of our miniature languages affect individual learning and their stability across multiple network generations. The results draw a mixed picture. On the one hand, neural networks show a strong tendency to avoid long-distance dependencies. On the other hand, there is no clear preference for the efficient, non-redundant encoding of information that is widely attested in natural language. We thus suggest inoculating a notion of ``effort'' into neural networks, as a possible way to make their linguistic behavior more human-like.},
}


@misc{IsmaelAnalysis,
	author = {{Ismael Garrido-Mu√±oz}},
	title = {Analysis, {Detection} and {Mitigation} of {Biases} in {Deep} {Learning} {Language} {Models}},
	abstract = {Recent advances in artificial intelligence have made it possible to make our everyday lives better, from apps that translate with great accuracy, to search engines that understand your query, to virtual assistants that answer your questions. However, these models capture the biases present in society and incorporate them into their knowledge. These biases and prejudices appear in a multitude of applications and systems. Given the same context, the model will have vastly different results depending on attributes such as the subject\textquoteright{}s gender, race or religion. In this thesis we focus on natural language bias in neural networks. However, bias is present in many areas of artificial intelligence. This behaviour is pervasive, first the models capture these associations and then replicate them as a result of their application. Bias in AI is encompassed in study areas such as Fairness or Explainability},
}


@inproceedings{Fine2014Biases,
	author = {Fine, Alex B. and Frank, Austin F. and Jaeger, T. Florian and Van Durme, Benjamin},
	booktitle = {Proceedings of the 52nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: Short {Papers})},
	year = {2014},
	organization = {Association for Computational Linguistics},
	title = {Biases in {Predicting} the {Human} {Language} {Model}},
	url = {http://dx.doi.org/10.3115/v1/P14-2002},
	doi = {10.3115/v1/p14-2002},
	abstract = {We consider the prediction of three human behavioral measures  lexical decision, word naming, and picture naming  through the lens of domain bias in language modeling. Contrasting the predictive ability of statistics derived from 6 different corpora, we find intuitive results showing that, e.g., a British corpus overpredicts the speed with which an American will react to the words ward and duke, and that the Google n-grams overpredicts familiarity with technology terms. This study aims to provoke increased consideration of the human language model by NLP practitioners: biases are not limited to differences between corpora (i.e. ``train'' vs. ``test''); they can exist as well between corpora and the intended user of the resultant technology.},
}


@inproceedings{Zhang2019Selection,
	author = {Zhang, Guanhua and Bai, Bing and Liang, Jian and Bai, Kun and Chang, Shiyu and Yu, Mo and Zhu, Conghui and Zhao, Tiejun},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	year = {2019},
	organization = {Association for Computational Linguistics},
	title = {Selection {Bias} {Explorations} and {Debias} {Methods} for {Natural} {Language} {Sentence} {Matching} {Datasets}},
	url = {http://dx.doi.org/10.18653/v1/P19-1435},
	doi = {10.18653/v1/p19-1435},
	abstract = {Natural Language Sentence Matching (NLSM) has gained substantial attention from both academics and the industry, and rich public datasets contribute a lot to this process. However, biased datasets can also hurt the generalization performance of trained models and give untrustworthy evaluation results. For many NLSM datasets, the providers select some pairs of sentences into the datasets, and this sampling procedure can easily bring unintended pattern, i.e., selection bias. One example is the QuoraQP dataset, where some content-independent naive features are unreasonably predictive. Such features are the reflection of the selection bias and termed as the ``leakage features.'' In this paper, we investigate the problem of selection bias on six NLSM datasets and find that four out of them are significantly biased. We further propose a training and evaluation framework to alleviate the bias. Experimental results on QuoraQP suggest that the proposed framework can improve the generalization ability of trained models, and give more trustworthy evaluation results for real-world adoptions.},
}


@inproceedings{Rudinger2017Social,
	author = {Rudinger, Rachel and May, Chandler and Van Durme, Benjamin},
	booktitle = {Proceedings of the {First} {ACL} {Workshop} on {Ethics} in {Natural} {Language} {Processing}},
	year = {2017},
	organization = {Association for Computational Linguistics},
	title = {Social {Bias} in {Elicited} {Natural} {Language} {Inferences}},
	url = {http://dx.doi.org/10.18653/v1/W17-1609},
	doi = {10.18653/v1/w17-1609},
	abstract = {We analyze the Stanford Natural Language Inference (SNLI) corpus in an investigation of bias and stereotyping in NLP data. The SNLI human-elicitation protocol makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples.},
}


@inproceedings{Karimi2020End,
	author = {Karimi Mahabadi, Rabeeh and Belinkov, Yonatan and Henderson, James},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	year = {2020},
	organization = {Association for Computational Linguistics},
	title = {End-to-{End} {Bias} {Mitigation} by {Modelling} {Biases} in {Corpora}},
	url = {http://dx.doi.org/10.18653/v1/2020.acl-main.769},
	doi = {10.18653/v1/2020.acl-main.769},
	abstract = {Several recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios. We propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets. The biases are specified in terms of one or more bias-only models, which learn to leverage the dataset biases. During training, the bias-only models\textquoteright{} predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing the training on the hard examples. We experiment on large-scale natural language inference and fact verification benchmarks, evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data. Results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets. Our code and data are publicly available in https://github.com/rabeehk/robust-nli.},
}


@inproceedings{Karimi2020End,
	author = {Karimi Mahabadi, Rabeeh and Belinkov, Yonatan and Henderson, James},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	year = {2020},
	organization = {Association for Computational Linguistics},
	title = {End-to-{End} {Bias} {Mitigation} by {Modelling} {Biases} in {Corpora}},
	url = {http://dx.doi.org/10.18653/v1/2020.acl-main.769},
	doi = {10.18653/v1/2020.acl-main.769},
	abstract = {Several recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios. We propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets. The biases are specified in terms of one or more bias-only models, which learn to leverage the dataset biases. During training, the bias-only models\textquoteright{} predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing the training on the hard examples. We experiment on large-scale natural language inference and fact verification benchmarks, evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data. Results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets. Our code and data are publicly available in https://github.com/rabeehk/robust-nli.},
}


@inproceedings{Ravfogel2019Studying,
	author = {Ravfogel, Shauli and Goldberg, Yoav and Linzen, Tal},
	booktitle = {Proceedings of the 2019 {Conference} of the {North}},
	year = {2019},
	organization = {Association for Computational Linguistics},
	title = {Studying the {Inductive} {Biases} of},
	url = {http://dx.doi.org/10.18653/v1/N19-1356},
	doi = {10.18653/v1/n19-1356},
	abstract = {How do typological properties such as word order and morphological case marking affect the ability of neural sequence models to acquire the syntax of a language? Cross-linguistic comparisons of RNNs\textquoteright{} syntactic performance (e.g., on subject-verb agreement prediction) are complicated by the fact that any two languages differ in multiple typological properties, as well as by differences in training corpus. We propose a paradigm that addresses these issues: we create synthetic versions of English, which differ from English in one or more typological parameters, and generate corpora for those languages based on a parsed English corpus. We report a series of experiments in which RNNs were trained to predict agreement features for verbs in each of those synthetic languages. Among other findings, (1) performance was higher in subject-verb-object order (as in English) than in subject-object-verb order (as in Japanese), suggesting that RNNs have a recency bias; (2) predicting agreement with both subject and object (polypersonal agreement) improves over predicting each separately, suggesting that underlying syntactic knowledge transfers across the two tasks; and (3) overt morphological case makes agreement prediction significantly easier, regardless of word order.},
}


@misc{StanleyBuilding,
	author = {{Stanley F. Chen}},
	title = {Building {Probabilistic} {Models} for {Natural} {Language}},
	abstract = {Building models of language is a central task in natural language processing. Traditionally, language has been modeled with manually-constructed grammars that describe which strings are grammatical and which are not; however, with the recent availability of massive amounts of on-line text, statistically-trained models are an attractive alternative. These models are generally probabilistic, yielding a score reflecting sentence frequency instead of a binary grammaticality judgement. Probabilistic models of language are a fundamental tool in speech recognition for resolving acoustically ambiguous utterances. For example, we prefer the transcription forbear to four bear as the former string is far more frequent in English text. Probabilistic models also have application in optical character recognition, handwriting recognition, spelling correction, part-of-speech tagging, and machine translation.

In this thesis, we investigate three problems involving the probabilistic modeling of language: smoothing n-gram models, statistical grammar induction, and bilingual sentence alignment. These three problems employ models at three different levels of language; they involve word-based, constituent-based, and sentence-based models, respectively. We describe techniques for improving the modeling of language at each of these levels, and surpass the performance of existing algorithms for each problem. We approach the three problems using three different frameworks. We relate each of these frameworks to the Bayesian paradigm, and show why each framework used was appropriate for the given problem. Finally, we show how our research addresses two central issues in probabilistic modeling: the sparse data problem and the problem of inducing hidden structure.},
}


@misc{CiprianExploiting,
	author = {{Ciprian Chelba}},
	title = {Exploiting {Syntactic} {Structure} for {Natural} {Language} {Modeling}},
	abstract = {The thesis presents an attempt at using the syntactic structure in natural language for improved language models for speech recognition. The structured language model merges techniques in automatic parsing and language modeling using an original probabilistic parameterization of a shift-reduce parser. A maximum likelihood reestimation procedure belonging to the class of expectation-maximization algorithms is employed for training the model. Experiments on the Wall Street Journal, Switchboard and Broadcast News corpora show improvement in both perplexity and word error rate - word lattice rescoring - over the standard 3-gram language model. The significance of the thesis lies in presenting an original approach to language modeling that uses the hierarchical - syntactic - structure in natural language to improve on current 3-gram modeling techniques for large vocabulary speech recognition.},
}


@article{Culbertson2012Bayesian,
	author = {Culbertson, Jennifer and Smolensky, Paul},
	journal = {Cognitive Science},
	number = {8},
	year = {2012},
	month = {sep 10},
	pages = {1468--1498},
	publisher = {Wiley},
	title = {A {Bayesian} {Model} of {Biases} in {Artificial} {Language} {Learning}: The {Case} of a {Word}-{Order} {Universal}},
	volume = {36},
	url = {http://dx.doi.org/10.1111/j.1551-6709.2012.01264.x},
	doi = {10.1111/j.1551-6709.2012.01264.x},
	abstract = {In this article, we develop a hierarchical Bayesian model of learning in a general type of artificial language-learning experiment in which learners are exposed to a mixture of grammars representing the variation present in real learners' input, particularly at times of language change. The modeling goal is to formalize and quantify hypothesized learning biases. The test case is an experiment (Culbertson, Smolensky, & Legendre, 2012) targeting the learning of word-order patterns in the nominal domain. The model identifies internal biases of the experimental participants, providing evidence that learners impose (possibly arbitrary) properties on the grammars they learn, potentially resulting in the cross-linguistic regularities known as typological universals. Learners exposed to mixtures of artificial grammars tended to shift those mixtures in certain ways rather than others; the model reveals how learners' inferences are systematically affected by specific prior biases. These biases are in line with a typological generalization-Greenberg's Universal 18-which bans a particular word-order pattern relating nouns, adjectives, and numerals.},
}


@inproceedings{Zhou2020Towards,
	author = {Zhou, Xiang and Bansal, Mohit},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	year = {2020},
	organization = {Association for Computational Linguistics},
	title = {Towards {Robustifying} {NLI} {Models} {Against} {Lexical} {Dataset} {Biases}},
	url = {http://dx.doi.org/10.18653/v1/2020.acl-main.773},
	doi = {10.18653/v1/2020.acl-main.773},
	abstract = {While deep learning models are making fast progress on the task of Natural Language Inference, recent studies have also shown that these models achieve high accuracy by exploiting several dataset biases, and without deep understanding of the language semantics. Using contradiction-word bias and word-overlapping bias as our two bias examples, this paper explores both data-level and model-level debiasing methods to robustify models against lexical dataset biases. First, we debias the dataset through data augmentation and enhancement, but show that the model bias cannot be fully removed via this method. Next, we also compare two ways of directly debiasing the model without knowing what the dataset biases are in advance. The first approach aims to remove the label bias at the embedding level. The second approach employs a bag-of-words sub-model to capture the features that are likely to exploit the bias and prevents the original model from learning these biased features by forcing orthogonality between these two sub-models. We performed evaluations on new balanced datasets extracted from the original MNLI dataset as well as the NLI stress tests, and show that the orthogonality approach is better at debiasing the model while maintaining competitive overall accuracy.},
}


@inproceedings{Huang2020Reducing,
	author = {Huang, Po-Sen and Zhang, Huan and Jiang, Ray and Stanforth, Robert and Welbl, Johannes and Rae, Jack and Maini, Vishal and Yogatama, Dani and Kohli, Pushmeet},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: EMNLP 2020},
	year = {2020},
	organization = {Association for Computational Linguistics},
	title = {Reducing {Sentiment} {Bias} in {Language} {Models} via {Counterfactual} {Evaluation}},
	url = {http://dx.doi.org/10.18653/v1/2020.findings-emnlp.7},
	doi = {10.18653/v1/2020.findings-emnlp.7},
	abstract = {Advances in language modeling architectures and the availability of large text corpora have driven progress in automatic text generation. While this results in models capable of generating coherent texts, it also prompts models to internalize social biases present in the training corpus. This paper aims to quantify and reduce a particular type of bias exhibited by language models: bias in the sentiment of generated text. Given a conditioning context (e.g., a writing prompt) and a language model, we analyze if (and how) the sentiment of the generated text is affected by changes in values of sensitive attributes (e.g., country names, occupations, genders) in the conditioning context using a form of counterfactual evaluation. We quantify sentiment bias by adopting individual and group fairness metrics from the fair machine learning literature, and demonstrate that large-scale models trained on two different corpora (news articles, and Wikipedia) exhibit considerable levels of bias. We then propose embedding and sentiment prediction-derived regularization on the language model\textquoteright{}s latent representations. The regularizations improve fairness metrics while retaining comparable levels of perplexity and semantic similarity.},
}


@inproceedings{Wen2015Semantically,
	author = {Wen, Tsung-Hsien and Gasic, Milica and Mrk{\v s}i{\' c}, Nikola and Su, Pei-Hao and Vandyke, David and Young, Steve},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	year = {2015},
	organization = {Association for Computational Linguistics},
	title = {Semantically {Conditioned} {LSTM}-based {Natural} {Language} {Generation} for {Spoken} {Dialogue} {Systems}},
	url = {http://dx.doi.org/10.18653/v1/D15-1199},
	doi = {10.18653/v1/d15-1199},
	abstract = {\copyright{} 2015 Association for Computational Linguistics. Natural language generation (NLG) is a critical component of spoken dialogue and it has a significant impact both on usability and perceived quality. Most NLG systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language. They are also not easily scaled to systems covering multiple domains and languages. This paper presents a statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure. The LSTM generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion, and language variation can be easily achieved by sampling from output candidates. With fewer heuristics, an objective evaluation in two differing test domains showed the proposed method improved performance compared to previous methods. Human judges scored the LSTM system higher on informativeness and naturalness and overall preferred it to the other systems. .},
}
