<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    {:root {
  --gray-bg: hsl(0, 0%, 97%);
  --gray-border: hsla(0, 0%, 0%, 0.1);
  --gray: rgba(0, 0, 0, 0.6);
  --border-radius: 5px;
  --orange: hsl(24, 100%, 50%);
  --distill-blue: hsl(200, 50%, 25%);
  --blue: #337699;
  --green: #3db867;
}

.subgrid {
  grid-column: screen;
  display: grid;
  grid-template-columns: inherit;
  grid-template-rows: inherit;
  grid-column-gap: inherit;
  grid-row-gap: inherit;
}

d-figure.base-grid {
  grid-column: screen;
  background: hsl(0, 0%, 97%);
  padding: 20px 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

d-figure {
  margin-bottom: 1em;
  position: relative;
}

d-figure>figure {
  margin-top: 0;
  margin-bottom: 0;
}

a.figure-number::before, a.figure-number-text::before {
  content: "Figure ";
}

a.table-number::before, a.table-number-text::before {
  content: "Table ";
}

a.video-number::before, a.video-number-text::before {
  content: "Video ";
}

a.figure-number, a.table-number, a.video-number {
  border-bottom-color: hsla(206, 90%, 20%, 0.3);
  text-transform: uppercase;
  font-size: .85em;
  color: hsla(206, 90%, 20%, 0.7);
}

a.figure-number:hover, a.table-number:hover, a.video-number:hover {
  border-bottom-width: 1px;
  border-bottom-style: solid;
  border-bottom-color: hsla(206, 90%, 20%, 0.7);
}

a.figure-number-text, a.table-number-text, a.video-number-text {
  border-bottom-color: hsla(206, 90%, 20%, 0.3);
  color: hsla(206, 90%, 20%, 0.7);
}

a.figure-number-text:hover, a.table-number-text:hover, a.video-number-text:hover {
  border-bottom-width: 1px;
  border-bottom-style: solid;
  border-bottom-color: hsla(206, 90%, 20%, 0.7);
}

.pointer {
  position: absolute;
  width: 26px;
  height: 26px;
  top: 26px;
  left: -48px;
}

.tk {
  color: red;
}

.hidden-citations {
  display: none;
}

/* TOC */

#contents {
  border-top: 1px solid #eee;
  padding: 1.5rem 0;
}

/* video examples */

.video-d-figure {
  float: right;
  position: relative;
  left: 11em;
  margin-left: -11em;
  width: 450px;
  padding-left: 1em;
  margin-bottom: 0em !important;
  padding-bottom: 1em;
}

.interactive-container {
  background-color: var(--gray-bg);
  border: 1px solid var(--gray-border);
  border-radius: var(--border-radius);
  /* border-bottom: 1px solid hsla(0, 0%, 0%, 0.1); */
  padding: 1em 1.5em;
  margin-bottom: 1em;
}

@media (max-width: 1180px) {
  .video-d-figure {
    float: none;
    left: 0;
    margin-left: none;
    padding-left: none;
    width: 100%;
  }
}

@media (max-width: 768px) {
  .interactive-container {
    border-radius: 0;
    padding: 1em;
  }
}

/* vtoc square */

.square-wrapper {
  grid-column: 3 / 4;
  margin-bottom: 1em;
  display: flex;
  flex-direction: column;
  justify-content: flex-end;
}

.square {
  height: 10px;
  width: 10px;
  margin-left: auto;
  border-radius: 1;
  margin-top: -20px;
  /* border: solid 1px #999; */
  background: none;
  border-radius: 50%;
}

.square-inner {
  /* height: 12px;
  width: 12px; */
  /* background: white; */
  /* margin: 3px; */
  /* border-radius: 3px; */
}

h3.affordance {
  /* line-height: 1.25em; */
  /* margin: 2rem 0 1.5rem 0; */
  border-bottom-width: 1px;
  border-bottom-style: solid;
  padding-bottom: 1rem;
}

h3.affordance.blue-green {
  border-bottom-color: #5AD86A;
}

h3.affordance.red-orange {
  border-bottom-color: #C7BF38;
}

h3.affordance.soft-blue {
  border-bottom-color: #73C4F5
}

h3.affordance.argon {
  border-bottom-color: #DF80F1
}

h3.affordance.sun {
  border-bottom-color: #E28765
}

@media (max-width: 1000px) {
  .square-wrapper {
    grid-column: 1 / 2;
  }
}

@media (max-width: 768px) {
  .square-wrapper {
    grid-column: 1 / 2;
    margin-bottom: 0;
  }
  .square {
    margin-left: 4px;
    width: 3px;
    height: 75%;
  }
  .square-inner {
    height: 0;
    width: 0;
    background: none;
    margin: 0;
    border-radius: 0;
  }
}

#affordances {
  /* grid-column: 4/7; */
}

#affordances>ul {
  list-style: none;
  line-height: 1rem;
}

#affordances>ul>li::before {
  content: "\2022";
  /* color: red; */
  font-weight: bold;
  display: inline-block;
  width: 1em;
  margin-left: -1em;
}

.blue-green::before {
  /* border: solid 2px #5AD86A; */
  color: #5AD86A;
}

.red-orange::before {
  /* border: solid 2px #C7BF38; */
  color: #C7BF38;
}

.soft-blue::before {
  /* border: solid 2px #73C4F5; */
  color: #73C4F5;
}

.argon::before {
  /* border: solid 2px #DF80F1; */
  color: #DF80F1;
}

.sun::before {
  /* border: solid 2px #E28765; */
  color: #E28765;
}

#message {
  grid-column: 7/12;
  padding: 1rem;
  border-left: 1px solid var(--gray);
  color: var(--gray);
  background-color: var(--gray-bg);
  display: flex;
  flex-direction: column;
  justify-content: center;
}

@media (max-width: 1180px) {
  #affordances {
    /* grid-column: text; */
    /* grid-column: 2/7; */
  }
  #message {
    /* grid-column: 7/14; */
    /* grid-column: text; */
  }
}

@media (max-width: 1000px) {}

@media(max-width: 768px) {}}
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
</head>


<body>
  <d-front-matter>
    <script type="text/json">
      {
    "title": "Cultural Biases in BERT: A primer on mitigating biases",
    "description": "Examining the hidden bias in a popular widely used Natural language processing model.",
    "authors": [
        {
            "author": "Gaurish Katlana",
            "authorURL": "https://katlana.com",
            "affiliation": "Minerva University",
            "affiliationURL": "https://www.minerva.edu/"
        }
    ],
    "katex": {
        "delimiters": [
            {
                "left": "$",
                "right": "$",
                "display": false
            },
            {
                "left": "$$",
                "right": "$$",
                "display": true
            }
        ]
    }
}
    </script>
  </d-front-matter>

  <d-title>
    <h1>Cultural Biases in BERT: A primer on mitigating biases</h1>
    <p>Examining the hidden bias in a popular widely used Natural language processing model.</p>
  </d-title>

  <d-byline></d-byline>

  <d-article>
    <d-contents id="toc"></d-contents>

    <div id="unmasker">
    </div>

    <!-- INTRO -->
    <!---Changing this would change change the written contents of the page-->
    <div>         
      <script type="module" src="https://js.withorbit.com/orbit-web-component.js"></script>
      <orbit-reviewarea color="lime">
        <orbit-prompt
          question="What is the name of the model we are going to discuss?"
          answer="BERT"
        ></orbit-prompt>
        <orbit-prompt
          question="Which year was BERT realesed?"
          answer="2018."
        ></orbit-prompt>
        <orbit-prompt
          question="What structure does BERT use?"
          answer="Transformers"
        ></orbit-prompt>

      </orbit-reviewarea>

      <h2 id="abstract">Abstract</h2>
      <p>The paper reviews the current methods for measuring biases in NLP models, starting with a literature review of group biases and specific techniques for measuring them. We describe the limitations of a novel metric called the Categorical Bias score and propose improvements to it, including weighing attributes and interaction effects between different characteristic groups. We test implicit association test methods by exploiting the hidden and attention states. We build upon the current bias evaluation and mitigation standard and propose a framework for evaluating NLP models during the building and pre-deployment phases. We find that the interaction biases are insignificant for most interactions, especially anything above two interactions that seem to have a negligible bias. Except for the interaction between the country of origin and language. Further, attribute weighting reduces the categorical bias score for multiple categories.</p>
      <h2 id="introduction">Introduction</h2>
      <p>In 2018, a group of researchers at Google AI Language group released BERT (Bidirectional Encoder Representations from Transformers) <dt-cite key="cite1"></dt-cite>. BERT is an NLP model that uses a Transformer architecture to learn the representation of words in a text <dt-cite key="cite2"></dt-cite>. The paper received 49885 citations as of October 16, 2022, representing the popularity and importance of this model <dt-cite key="cite3"></dt-cite>. The model is freely available for download and found uses in various applications, including question-answering <dt-cite key="cite4"></dt-cite><dt-cite key="cite5"></dt-cite><dt-cite key="cite6"></dt-cite>, text classification <dt-cite key="cite7"></dt-cite><dt-cite key="cite8"></dt-cite><dt-cite key="cite9"></dt-cite>, and text generation <dt-cite key="cite10"></dt-cite> <dt-cite key="cite11"></dt-cite>. This model represents a broader trend in the NLP community to use large pre-trained models for various tasks and use transfer learning to fine-tune these models for specific tasks <dt-cite key="cite12"></dt-cite>.</p>
      <p>The model was found to be racially and ethnically biased and displays a strong gender preference <dt-cite key="cite13"></dt-cite>. Further, as AI adoption is gaining traction and BERT usage is increasing, it is essential to understand the biases in the model and how to mitigate them. BERT has a submodel named 'BERT-uncased-English,' a model trained on English Wikipedia and BooksCorpus. English Wikipedia is a large dataset of all English articles on Wikipedia, a free online encyclopedia used by over 44 million people and containing over 6.7 million articles. BooksCourpus is a dataset of 11,038 novel books written by unpublished and self-published authors. Both the training datasets might contain biases <dt-cite key="cite14"></dt-cite>.</p>
      <p>We will start with an analogy to biases in our society today. Biases can arise on the neurobiological, cognitive, and societal levels<dt-cite key="cite14"></dt-cite>. On the neurobiological level, biases may be influenced by factors such as genetics <dt-cite key="cite15"></dt-cite>, brain’s structure <dt-cite key="cite10"></dt-cite> and function, hormonal influences <dt-cite key="cite15.1"></dt-cite>. For example, studies have found that certain genetic variations can contribute to biases in social perception and decision-making <dt-cite key="cite16"></dt-cite>. Additionally, research has shown that the structure and function of certain brain regions, such as the amygdala and prefrontal cortex, can play a role in the formation and expression of biases <dt-cite key="cite17"></dt-cite>.</p><dt-cite key="cite18"></dt-cite>.</p>.</p>
      <p>On the cognitive level, biases can arise through processes such as heuristics, which are mental shortcuts that people use to make judgments and decisions quickly and efficiently. These heuristics can lead to biases by providing simplified but sometimes inaccurate or incomplete information about the world. For example, people may use stereotypes as heuristics to quickly assess a person or situation, which can lead to biases against certain groups of people.</p>
      <p>On the societal level, biases can arise through the influence of culture, social norms, and other factors that shape people's beliefs and attitudes. For example, biases against certain groups of people may be reinforced by societal attitudes and beliefs about those groups, such as stereotypes or prejudice. Additionally, stereotypes can be perpetuated by the ways in which people interact with each other and the world, such as through media, education, and social institutions <dt-cite key="cite19"></dt-cite>.</p>
      <h2 id="section-1-understanding-culture">Section 1: Understanding Culture</h2>
      <p>Defining culture is a challenging task. Culture has no precise definition, but one can use multiple definitions that can be used in many different contexts. Culture can be broken down into multiple components: language, traditions, religious beliefs, spiritual beliefs, food, music, and many others. However, all the cultural elements are passed through communication between people. <dt-cite key="cite20"></dt-cite>.</p>
      <p>People can belong to multiple cultural sub-elements and create their own cultural identities. Culture is dynamic with respect to places, people, and many other factors. Over the past millennia, culture has changed drastically and will continue to change in the future. We want to focus on how culture plays a role in the English language. The exploration of cultural changes is beyond the scope of this project. Multiple cultural groups are discriminated against.</p>
      <h3 id="1-different-varieties-of-english">1. Different Varieties of English</h3>
      <p>When we talk about English, it is almost not a single language. There are 1.5 billion people that speak English. <dt-cite key="cite21"></dt-cite> At least 100 dialects, accents, and English variations are spoken worldwide <dt-cite key="cite22"></dt-cite>. The most common dialects of English are American English, British English, Australian English, and Indian English. These dialects are not mutually intelligible and are different in many ways. For example, the word &quot;color&quot; is spelled as &quot;colour&quot; in British English and &quot;color&quot; in American English. The word &quot;theater&quot; is pronounced as &quot;theatre&quot; in Indian English and &quot;theater&quot; in American English. There are also variations like Black English Vernacular, Cockney English, and many others dialects spoken by minorities around the world that are considered vulgar by mainstream English speakers. Many slang words used in African-American Vernacular English are considered vulgar in standard English dialects <dt-cite key="cite23"></dt-cite>.</p>
      <p>This mainstream view of minorities’ dialects has enormous implications for minorities. The social stigma creates social pressure that the way they speak is vulgar and marginalizes them (<dt-cite key="cite24"></dt-cite><dt-cite key="cite25"></dt-cite>). Most NLP models are trained on English text that does not include substantial portion of African American Vernacular English <dt-cite key="cite26"></dt-cite>. A similar bias is observed in the models. It seems like the models are “white-washed” because of these biases. The deployed models might marginalize speakers of these dialects and create a vicious cycle that hurts these minorities.</p>
      <h3 id="2-considering-other-lingual-speakers">2. Considering other lingual speakers</h3>
      <p>Languages can also define culture. Language is an integral part of the culture as it dictates the way people communicate with each other. It leads to the spread of ideas, beliefs, and values. Language can reflect various ideologies and values a culture holds. Texts written in specific languages might hold biases or negative connotations for certain groups of people. This existence of bias is a massive problem as it creates a bias in how models think about other cultures. For instance, when we ask BERT to identify a hypothetical Spanish speaker's religion, it predicts that the religion should be Christianity. Hence, the model implies causal relationships that might play into existing stereotypes.</p>
      <p>This result can have severe implications in real-life situations as NLP models are deployed to help people in their daily lives. For example, if a Spanish speaker is looking for necklaces, the model will implicitly assume that they are a Christian and suggest a crucifix.</p>
      <h3 id="3-considering-food">3. Considering food</h3>
      <p>Food is an integral part of our lives; everyone needs food to survive. Hence, many cultures worldwide developed different cuisines, flavors, and dishes. Food is also an identity. It brings people together and creates a sense of belonging. As we saw in the case of language communities above, the model also carries biases for a specific cuisine. For instance, eating with hands is typical in South Asian cultures, but it is considered rude in western cultures. Hence, many models tend to assume that eating with hands is rude and will not suggest it to the user.
      In addition, if a natural language model with food biases is used in applications such as machine translation or customer service, it could lead to unfair or inappropriate responses to users with different food-related customs or preferences. This bias could create a negative impression of the model and the application it is used in and may even lead to discrimination or prejudice against individuals or groups based on their food preferences.
      Overall, developers of natural language models need to be aware of potential food biases in their training data and take steps to mitigate them, such as using a diverse and representative training data set. These steps can help the model better understand and represent the nuances of different cultures and reduce the potential for bias in its predictions.</p>
      <h3 id="4-considering-religion">4. Considering religion</h3>
      <p>Religion is another essential part of the culture. It is a set of beliefs and practices that a group follows. For example, in Hinduism, cows are believed to be sacred and should not be harmed <dt-cite key="cite27"></dt-cite>. In Islam, eating pork is forbidden. In Christianity, spreading the word of Jesus is considered a moral duty. These beliefs are passed down through generations and shape how people think and behave. There are also preconceived notions about people from different religions. There exist many tensions between various religious groups, and a biased model usage can exaggerate these tensions instead of addressing them appropriately. For instance, when we asked BERT about what a Christian carried out, it replied with suicide and murder in the case of a Muslim. With some prompts. From we also found that the model believed there was a preconceived notion about hatred towards other religions.</p>
      <p>Another example is a biased natural language model used in a social media platform to moderate content and flag inappropriate or offensive posts. Suppose the model has a bias against a specific religion. In that case, it could unfairly flag or remove posts that contain references to that religion, even if they are not inappropriate or offensive. This action could lead to censorship or discrimination against individuals or groups based on religious beliefs, potentially affecting the platform's reputation and user relationships.</p>
      <h3 id="5-considering-nationality">5. Considering nationality</h3>
      <p>Nationality also plays a vital role in culture. Countries are usually formed based on shared beliefs, religion, language, land, and other factors. Hence, most people associate strongly with their nationality or, in some cases, do not want to be associated with the nationality due to generational trauma, the political climate of their nation, etc. There are exceptions to having a strong sense of national identity, as there might be a difference between a person's nationality and the nationality one resonates with, creating nationality dysphoria.</p>
      <p>For example, consider a situation where a user from a country where English is not the primary language tries to use a machine translation application to translate a sentence from their native language into English. If the language model has a bias against the user's native language, it could provide a poor-quality translation that is full of errors or misunderstandings. This could lead to confusion or frustration for the user and potentially even affect their relationship with the person they are communicating with if the translation is inaccurate.</p>
      <h2 id="section-2-understanding-bert">Section 2: Understanding BERT</h2>
      <h3 id="21-introduction-to-bert">2.1 Introduction to BERT</h3>
      <p>BERT is a transformer; hence, it is vital to learn about Transformers before understanding BERT. In this section, we will learn about Transformers and how BERT is built.</p>
      <p>The model contains two main components.</p>
      <ol>
      <li>The encoder takes all the tokens at once and generates embeddings for each token simultaneously. The encoder helps the model to learn more about the context, grammar, and different semantics before using it for another task in the decoder layer.</li>
      <li>The decoder uses the encoder encodings to solve a specific task. For instance, it can be trained to translate a sentence from one language to another.</li>
      </ol>
      <p>The decoder can be trained to solve different tasks. For instance, it can be trained to solve classification, regression, or question-answering tasks. BERT is a stack of encoders and does not have a decoder.</p>
      <h3 id="22-input">2.2 Input</h3>
      <p><img style="width:100%" src="/images/Untitled.png" alt="Figure 1: The input to the BERT model made from different components. Here [SEP] token represents the full stop in the model. "></p>
      <p>Figure 1: The input to the BERT model made from different components. Here [SEP] token represents the full stop in the model.</p>
      <p>The input to the BERT model is made from three parts.</p>
      <ol>
      <li><strong>Token embeddings:</strong> To input sentences or paragraphs into BERT, we must break the sentence down into words or smaller units that BERT can understand. The problem with feeding the neural network with words is that English contains many words, and there is no ground source of truth with all the English words. Hence, to overcome this problem BERT tokenizer breaks down the sentences into tokens made from 30522 tokens available in WordPiece embeddings [2]. Each token is a vector of 768 dimensions. For instance, &quot;playing&quot; is broken down into &quot;play&quot; and &quot;##ing.&quot; This splitting of words into tokens reduces the vocabulary needed and makes the model more efficient. One cannot use any other tokenizer or word-to-vector library to input text into BERT.</li>
      <li><strong>Segment embeddings:</strong> To differentiate between two sentences or between question and answer, the BERT needs to understand which class or part of the input a word belongs to. For instance, if we are trying to solve a question-answering task, we can use this to differentiate between the question and the answer—usually represented by [SEP] token.</li>
      <li><strong>Position embeddings:</strong> This is used to differentiate between the token's position in the sentence. For instance, if we are trying to solve a question-answering task, we can use this to differentiate between the position of the question and the answer. We need position embeddings because we input all the tokens at once, and the model cannot know the token's position in the sentence. Hence, we need to explicitly tell the model the token's position in the sentence. Position embeddings inform the exact token number of a token in a given input. Position embeddings also help BERT learn the context. For instance, consider two sentences, &quot;A person was sitting on the bank of the river&quot; and &quot;A person robbed a bank.&quot; In the first sentence, the word &quot;bank&quot; refers to the river bank, and in the second sentence, the word &quot;bank&quot; refers to the financial institution or a bank. Hence, the model needs to learn the context of the word &quot;bank&quot; to solve the task.</li>
      </ol>
      <p>Currently, BERT can handle 512 tokens at once. For inputs larger than 512 tokens, the inputs can be broken down into smaller chunks, and the outputs of these chunks can be concatenated. However, in this case, the output would only contain part of the input context. The token limit is one of the most significant limitations of BERT. It also explains the word limit for google translate, which uses a similar transformer model.</p>
      <h3 id="23-the-model">2.3 The model</h3>
      <p>Attention allows BERT to focus on specific parts of the input text and weigh them accordingly when generating a response.</p>
      <p>Typically, an encoder has two parts: a multi-headed self-attention mechanism and a feedforward neural network.</p>
      <p>The self-attention mechanism allows the encoder to focus on specific parts of the input data and weigh their importance when generating the encoded representation. This is useful for capturing the relationships between different parts of the input data, such as the relationship between words in a sentence.</p>
      <p>Let's say that our attention heads need to output y after taking in the input x. The attentional layer simply calculates a weighted average for every element in x and outputs it to y.</p>
      <p>The feedforward neural network is a traditional, fully-connected neural network that takes the encoded representation produced by the self-attention mechanism as input and generates a new, more compact representation as output. This representation is then passed on to the rest of the network for further processing.</p>
      <p>Together, these two parts of the encoder allow a neural network to effectively process input data and generate a useful representation of it for downstream tasks.</p>
      <h3 id="24-output">2.4 Output</h3>
      <p>The output of BERT is divided into two parts.</p>
      <ol>
      <li>A token denoting what is the probability of the first part of the text following the second part of the text. The two text parts are separated by the CLS token.</li>
      <li>The probability distribution over all tokens. Every token has 30,000 possibilities and we output a probability distribution over all these possible tokens.</li>
      </ol>
      <p>The loss is calculated by cross-entropy loss between the predicted probability distribution of tokens and the actual probability distribution of tokens. The loss is then backpropagated to update the weights of the model.</p>
      <p>In this study, we will study the “best-base-uncased” model. &quot;Uncased&quot; means that the model does not differentiate between upper and lower case letters.</p>
      <h2 id="section-3-literature-review">Section 3: Literature review</h2>
      <p>The current state-of-the-art methods for measuring biases in NLP models include manual annotation, automated annotation, and automatic metrics. Manual annotation is time-consuming and costly but can provide detailed insight into potential biases<dt-cite key="cite28"></dt-cite>. Automated annotation is faster and more scalable than manual annotation but may be less accurate (<dt-cite key="cite29"></dt-cite><dt-cite key="cite30"></dt-cite>). Automatic metrics are the most efficient and cost-effective way to evaluate and measure bias in NLP models. They can quickly and accurately measure bias in a model and compare the performance of different models. This section will review the different methods used to identify bias in Natural language models. We will start with identifying biases in Word embeddings that BERT does not use but produces. The major disadvantage of using biased metrics is the problem of aligning our subjective beliefs about equality into mathematical form. Using a singular metric to determine the nature of the algorithm will often lose the granularity of using examples. For instance, metrics don’t actually differentiate between the degree of preference for multiple ethnic groups.  ****</p>
      <h3 id="31-using-word-embeddings-to-determine-bias">3.1 Using word embeddings to determine bias</h3>
      <p>Word embeddings are a way to represent words as vectors in a high-dimensional space allowing us to perform mathematical operations on them <dt-cite key="cite31"></dt-cite>. For instance, the vector for &quot;king&quot; and the vector that represents the word &quot;queen&quot; will be two different vectors. We can then find the vector that represents the word &quot;man&quot; and add it to the vector that represents the difference between &quot;king&quot; and &quot;queen&quot; to get the vector that represents the word &quot;women.&quot; Word embeddings are a compelling way to represent words and their relationships with each other.
      However, sometimes one group is closer to words that represent toxic or harmful intentions. Hence, we will find the distances between the clusters representing a specific cultural group and those representing toxic words . This method is applied to debias tokenizers that output vectors.</p>
      <h3 id="32-performing-bias-measurement-on-the-output">3.2 Performing bias measurement on the output</h3>
      <p><strong>3.2.1 Fairness Through Awareness (2011)</strong></p>
      <p>This paper describes fairness in classification algorithms as two individuals with similar classification dependent variables should be classified similarly. The Lipschitz condition requires that any two individuals x, y that are at distance d(x, y) ∈ [0, 1] map to distributions M(x) and M(y), respectively, such that the statistical distance between M(x) and M(y) is at most d(x, y). Rather than viewing this as an absolute condition, this paper tries to minimize the ratio of this difference. In other words, one should try to minimize</p>
      <p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>D</mi><mo stretchy="false">(</mo><mi>M</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>M</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><mrow><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">f(x,y) = \frac{D(M(x), M(y))}{d(x,y)}  
      </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">))</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
      <p>Similarly, this notion could be extended to all the people of a group that shares a common property that should not matter for classification and should receive the same positive classification rate as the population. For instance, gender should not affect the probability of being selected for a job; the probability that the male group gets chosen for the interview should be the same as the probability of the female group, which should be the same as the population as a whole.</p>
      <p>However, the authors also displayed that more than the described conditions are needed to be a sufficient measure of fairness. They pointed out that maintaining the same classification level between different groups might reduce the algorithm's utility, rendering it ineffective for the task at hand. Decision-makers can select an unqualified subset of a group and discriminate against them by ensuring to reach parity but not ensuring that they pass the matching criteria. One can also discriminate on non-identified subsets of groups. For instance, one can match the parity of men with the overall population but fail to do the same for cis-men and trans-men subgroups. There are multiple groups at work for any algorithm.</p>
      <p><strong>3.2.2 Learning Classification without Disparate Mistreatment (2017)</strong></p>
      <p>This paper introduced the three different notions of unfairness measurements.</p>
      <ol>
      <li><strong>Disparate Mistreatment:</strong> This is the difference in classification rate that occurs when two individuals share all the same attributes except a sensitive attribute. In theory, this would reflect the effect of the sensitive attribute on the output while controlling for all other variables. Hence, for a model to be an unbiased model has to satisfy the following property: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi mathvariant="normal">∣</mi><mi>a</mi><mo separator="true">,</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi mathvariant="normal">∣</mi><mi>a</mi><mo separator="true">,</mo><mover accent="true"><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>^</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi mathvariant="normal">∣</mi><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(\hat{y}|a,\hat{x}) = P(\hat{y}|a,\hat{x&#x27;}) = P(\hat{y}|a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2222em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1913em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9413em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6779em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span><span style="top:-3.2469em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span>, where ‘a’ are the other attributes and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo separator="true">,</mo><mi>x</mi><mtext>’</mtext></mrow><annotation encoding="application/x-tex">x, x’</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mord">’</span></span></span></span> are the different sensitive attributed and y is an outcome.</li>
      <li><strong>Disparate impact:</strong> This is an analog of group fairness measured in a probability. For instance, race should not impact hiring decisions; hence, for a job hiring algorithm, P(getting hired| race = black) = P(getting hired and having race = black).</li>
      <li><strong>Disparate mistreatment:</strong> To avoid disparate mistreatment in a binary classifier, the overall misclassification rate (OMR), false-positive rate (FPR), and false-negative rate (FNR) must all be the same across groups with different values of the sensitive feature z. The misclassification rate should be the same for both groups regardless of the value of z. If the misclassification rate is not the same for both groups, the model suffers from disparate mistreatment.</li>
      </ol>
      <p><strong>3.2.3 On Measuring Social Biases in Sentence Encoders (2019)</strong></p>
      <p>This paper extends on the Word embedding association test (WEAT) mentioned in section 3.1, but for variable length sentence encoded vectors. This method uses templates like &quot;This is <word>.&quot;, &quot;<word> is here.&quot; these templates do not have any meaning but are largely determined by the words we insert into them. In this test, we create sentence embeddings for sentences like &quot;Jamel is here.&quot; where Jamel is a predominantly African-American name, and similar embeddings for &quot;This is evil.&quot;, &quot;There is love,&quot; and compare the sentence embeddings for the two sentences.</p>
      <p>The two vector embeddings can then be used to find the cosine similarity or other similarity measures. We find that the models like BERT contain stereotypes like the Angry Black woman stereotype.</p>
      <p><strong>3.2.4 Mitigating Language-Dependent Ethnic Bias in BERT (2021)</strong></p>
      <p>This paper developed a metric called the Categorical Bias score. The score is based on the idea that the ratio between the probability of a category having a particular attribute and the probability of a category having any attribute should represent the affinity of the bias towards the attribute. For instance, for a token &quot;he,&quot; the two sentences &quot;[MASK] is a nurse&quot; and &quot;[MASK] is a [MASK]&quot; should give us P(he|nurse) and P(he) because the second sentence does not have any attribute. We can then apply cosine similarity between the log of the exciting categories.</p>
      <p><img style="width:100%" src="/images/Untitled%201.png" alt="Figure 2: Figure from the Mitigating Language-Dependent Ethnic Bias in BERT (2021). Subfigure a displays the process of obtaining the base rates between two target groups whereas subfigure b displays the categorical bias score from Ahn &amp; Oh, 2021 calculation. "></p>
      <p>Figure 2: Figure from the <strong>Mitigating Language-Dependent Ethnic Bias in BERT (2021). Subfigure a displays the process of obtaining the base rates between two target groups whereas subfigure b displays the categorical bias score from Ahn &amp; Oh, 2021 calculation.</strong></p>
      <p>The categorical bias score is the average score of multiple attributes over multiple templates that allow measuring the bias. If the number of templates is T, the number of attributes is A, and the number of categories (for instance, 2 for gender), we obtain the following formula for the Categorical bias score:</p>
      <p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mn>1</mn><mrow><mi>T</mi><mo>⋅</mo><mi>A</mi></mrow></mfrac><munder><mo>∑</mo><mrow><mi>t</mi><mo>∈</mo><mi>T</mi></mrow></munder><munder><mo>∑</mo><mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow></munder><mi>V</mi><mi>a</mi><msub><mi>r</mi><mrow><mi>n</mi><mo>∈</mo><mi>N</mi></mrow></msub><mi>l</mi><mi>o</mi><mi>g</mi><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>class n</mtext><mi mathvariant="normal">∣</mi><mtext>attribute A</mtext><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>class n</mtext><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{T\cdot A} \sum_{t \in T} \sum_{a \in A} Var_{n\in N} log \frac{P(\textrm{class n}| \textrm{attribute A})}{P(\textrm{class n})}
      </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.7487em;vertical-align:-1.3217em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">A</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8557em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3217em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8557em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight">A</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3217em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">Va</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord textrm">class n</span></span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord textrm">class n</span></span><span class="mord">∣</span><span class="mord text"><span class="mord textrm">attribute A</span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
      <p>The drawback of this metric is that this metric can only work with a single class and cannot utilize multiple classes. For instance, it cannot see the effect of the synergy of gender and racial bias that can be observed from the Sentence encoders. Further, using unmasking tasks to determine the base rate or prior rate might not be accurate, as we will explore later. Further, there is equality in comparison between different attributes, for instance, “terrorist” and “pick-pocketer” are bad attributes, but they are both considered equal in this metric.</p>
      <h3 id="33-findings-summary">3.3 Findings Summary</h3>
      <p>One of the essential things one can see from the patterns in Bias identification is the heavy reliance on having class similarity and the lack of interpretability of the models. Using class similarity fails to use implicit knowledge about sensitive attributes. For instance, “John is a doctor” is more likely than “Susan is a doctor” might rely on the fact that John is predominantly a male name. A model’s implicit knowledge allows models to perform well on zero-shot tasks such as fact fulfilling.</p>
      <p>We can argue that the algorithms only need to outperform humans on bias metrics in order to be deployed. However, algorithms are fast, cheap, and will be more widely used than human-based judgments. Hence, we would argue that outperforming humans should not be a valid metric.</p>
      <p>All metrics need to be revised to explain the reasoning behind the bias systematically. Further, the NLP models keep getting larger and less interpretable . Hence, it is a great time to interfere and make those models more interpretable.</p>
      <h2 id="section-4--bias-test-for-multiple-cultural-elements-using-modified-categorical-bias-score">Section 4:  Bias test for multiple cultural elements using modified Categorical Bias score.</h2>
      <h4 id="section-41-introduction">Section 4.1: Introduction</h4>
      <p>In this section, we would extend the idea of using multiple attributes and try to measure the synergetic energy between different classes. We will use the following template to measure the interaction bias,</p>
      <aside>
      💡 “[Name] is from [Country]. [Name] speaks [Language]. [Name] is {a/an} [Religion]. [Name] likes to eat [Dish]. [Name] listens to [Music Genre]. [Name] is {Attribute}. ”
      </aside>
      <p>In this template, we are using 5 major pieces of information (that are Location, Language, Religion, Cuisine, and Music) and a single attribute. We can also change the order of sentences in order to form new templates or change the way the information is represented in them.</p>
      <p>This would allow us to identify interference bias between different characteristics. We would use sentiment analysis of nltk that is based on a toxicity score to assign weights to the attributes. This would allow us to have a weight component for the attribute.</p>
      <p>The following below is the new modified metric:</p>
      <p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mn>1</mn><mrow><mi>T</mi><mo>⋅</mo><munder><mo>∑</mo><mrow><mi>w</mi><mo>∈</mo><mi>W</mi></mrow></munder><msub><mi>A</mi><mi>w</mi></msub></mrow></mfrac><munder><mo>∑</mo><mrow><mi>t</mi><mo>∈</mo><mi>T</mi></mrow></munder><munder><mo>∑</mo><mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow></munder><mi>V</mi><mi>a</mi><msub><mi>r</mi><mrow><mi>n</mi><mo>∈</mo><mi>N</mi></mrow></msub><mi>log</mi><mo>⁡</mo><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>class n</mtext><mi mathvariant="normal">∣</mi><mtext>attribute A</mtext><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>class n</mtext><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{T\cdot \sum_{w \in W} A_w} \sum_{t \in T} \sum_{a \in A} Var_{n\in N} \log \frac{P(\textrm{class n}| \textrm{attribute A})}{P(\textrm{class n})}
      </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.7487em;vertical-align:-1.3217em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1786em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3271em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0131em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8557em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3217em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8557em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight">A</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3217em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">Va</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord textrm">class n</span></span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord textrm">class n</span></span><span class="mord">∣</span><span class="mord text"><span class="mord textrm">attribute A</span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
      <p>We will also explore new methods to find the original base rate. Further, the P(class n) that represents the base rate would be determined by two new methods. First, we will try to find the occurrence rate in the data set that is simply defined by the number of times we observe the attribute in the dataset. Secondly, instead of using a multitasking task to determine the base rate, we will try multiple neutral attributes in order to determine the neutral base rate.</p>
      <p>The argument for using the neutral base rate is that there is a chance that there is bias in the multitasking method. For instance, if BERT is given the sentence “A person who is from [MASK] is a [MASK]”, BERT might predict P(”Canada”) as 0.01 and P(”doctor”) as 0.01 and P(”terrorist”) as 0.0001, similarly, in this fashion, we can see that the second mask is mostly positive and hence, the rate we obtain might be biased because “Canada is more positive”. We can explore this on a larger level. Instead, we would plan to use statements like “A person from [Country] is sleeping.”, “A person from [Country] is eating.”. These statements are neutral and don’t have a bias in them. This would allow us to obtain a neutral bias. Further, if BERT considers positive or negative sentiments for these words by averaging the probability we might be able to find the near-neutral sentiments.</p>
      <p>The next question that rises is how we measure interaction bias, between the different characteristics. One can see this problem is especially problematic as there are multiple interactions possible. For instance, there are $\binom{5}{2}$ways to select 2 characteristics out of 5 characteristics in the study. To study all the biases, one requires <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mi>n</mi></msup><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><mo fence="true">(</mo><mfrac linethickness="0px"><mi>n</mi><mi>i</mi></mfrac><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">2^n = \sum_{i=1}^n \binom{n}{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6644em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2em;vertical-align:-0.35em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7454em;"><span style="top:-2.355em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.144em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span>  bias measurements for n number of characteristics. Furthermore, as we increase the number of characteristics in interaction the time it requires to calculate the bias increases as well, hence, the overall time complexity for using a modified categorical bias score would be on the order of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><mo fence="true">(</mo><mfrac linethickness="0px"><mi>n</mi><mi>i</mi></mfrac><mo fence="true">)</mo></mrow><mo>⋅</mo><msup><mi>c</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">\sum_{i=1}^n \binom{n}{i} \cdot c^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2em;vertical-align:-0.35em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7454em;"><span style="top:-2.355em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.144em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8247em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span>, using binomial theorem we can prove this is an exponential algorithm with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>c</mi><mi>n</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(c^n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>, where c is the average number of characteristics.</p>
      <h3 id="results">Results</h3>
      <p>From running the tests we obtain the following results.</p>
      <h3 id="single-characteristics">Single characteristics</h3>
      <h4 id="categorical-bias-score-for-single-characteristics">Categorical Bias score for single characteristics</h4>
      <p><img style="width:100%" src="/images/Untitled%202.png" alt="Figure 3: The single characteristic categorical bias score for Country, Religion, Music, and Food aspects (Top left to bottom right). "></p>
      <p>Figure 3: The single characteristic categorical bias score for Country, Religion, Music, and Food aspects (Top left to bottom right).</p>
      <p>We can see here that the country has the highest categorical score followed by religion, food preference, and music.</p>
      <h3 id="section-43-bias-for-individual-characteristics">Section 4.3 Bias for individual characteristics</h3>
      <p>In this section, we will look at the results of applying the modified categorical bias for five categories location, language, religion, cuisine, and music genre. We would look at them by using a single-sentence template from above.</p>
      <h4 id="weighted-attributes-score">Weighted attributes score</h4>
      <p>Here we can see clearly that the weighted attributes have a lower score in all the aspects. This is expected because the weighting for the adjectives has values less than one for all adjectives. Hence,</p>
      <p><img style="width:100%" src="/images/Untitled%203.png" alt="Figure 4: The single characteristic weighted/modified categorical bias score for Country, Religion, Music, and Food aspects (Top left to bottom right). "></p>
      <p>Figure 4: The single characteristic weighted/modified categorical bias score for Country, Religion, Music, and Food aspects (Top left to bottom right).</p>
      <h3 id="section-43-bias-for-interaction-between-characteristics">Section 4.3 Bias for interaction between characteristics</h3>
      <p>In this section, we will find the interactions bias. As we saw calculating the complete interaction bias is an exponential problem and is not feasible without sampling. Hence, rather than exploring the complete interaction bias space between the characteristics, we will use a sampling method to sample from it. We will select fewer adjectives and the characteristics at combo. We will use stratified sampling using sentiment scores to reduce the number of adjectives to 12 adjectives and the number of characteristics would be reduced 10-fold.</p>
      <p>After sampling the number of characteristics, we get the following results after the stratified sampling was carried out</p>
      <table>
      <thead>
      <tr>
      <th>Characteristics</th>
      <th>After Sampling</th>
      <th>Pre Sampling</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>Country</td>
      <td>24</td>
      <td>245</td>
      </tr>
      <tr>
      <td>Language</td>
      <td>18</td>
      <td>185</td>
      </tr>
      <tr>
      <td>Dishes</td>
      <td>18</td>
      <td>184</td>
      </tr>
      <tr>
      <td>Religion</td>
      <td>4</td>
      <td>17</td>
      </tr>
      <tr>
      <td>Music</td>
      <td>4</td>
      <td>19</td>
      </tr>
      </tbody>
      </table>
      <p>Table 1: The sample sizes pre-sampling and post-sampling. Reflecting the decrease of 10% in the final testing dataset.</p>
      <h3 id="431-interaction-between-two-characteristics">4.3.1 Interaction between two characteristics</h3>
      <p><strong>Categorical bias</strong></p>
      <p><img style="width:100%" src="/images/Untitled%204.png" alt="Figure 5: Heat map of nonweighted categorical bias. Displaying interaction between two characteristics. The diagonal elements are 0.0 because the interaction score can only occur between two different characteristics and signify a null score. "></p>
      <p>Figure 5: Heat map of nonweighted categorical bias. Displaying interaction between two characteristics. The diagonal elements are 0.0 because the interaction score can only occur between two different characteristics and signify a null score.</p>
      <p><strong>Modified Categorical bias score</strong></p>
      <p><img style="width:100%" src="/images/Untitled%205.png" alt="Figure 6: Heat map of modified categorical bias for interaction between two characteristics. Displaying interaction between two characteristics. The diagonal elements are 0.0 because the interaction score can only occur between two different characteristics and signify a null score. "></p>
      <p>Figure 6: Heat map of modified categorical bias for interaction between two characteristics. Displaying interaction between two characteristics. The diagonal elements are 0.0 because the interaction score can only occur between two different characteristics and signify a null score.</p>
      <h3 id="432-interaction-between-three-characteristics">4.3.2 Interaction between three characteristics</h3>
      <h4 id="categorical-bias-score">Categorical bias score</h4>
      <p><img style="width:100%" src="/images/Untitled%206.png" alt="Figure 7: Representing the interaction bias between three different elements, the title of each heatmap displays the fixed characteristics and the other two characteristics are represented by the axes on the heat map. Here the row and column with the same characteristic as the title is left with a score of 0.0. Similarly, the diagram elements are left.  "></p>
      <p>Figure 7: Representing the interaction bias between three different elements, the title of each heatmap displays the fixed characteristics and the other two characteristics are represented by the axes on the heat map. Here the row and column with the same characteristic as the title is left with a score of 0.0. Similarly, the diagram elements are left.</p>
      <h4 id="modified-categorical-bias">Modified categorical bias</h4>
      <p><img style="width:100%" src="/images/Untitled%207.png" alt="Figure 8: Representing the modified/ weighted interaction bias between three different elements, the title of each heatmap displays the fixed characteristics and the other two characteristics are represented by the axes on the heat map. Here the row and column with the same characteristic as the title is left with a score of 0.0. Similarly, the diagram elements are left.  "></p>
      <p>Figure 8: Representing the modified/ weighted interaction bias between three different elements, the title of each heatmap displays the fixed characteristics and the other two characteristics are represented by the axes on the heat map. Here the row and column with the same characteristic as the title is left with a score of 0.0. Similarly, the diagram elements are left.</p>
      <h3 id="433-interaction-between-four-characteristics">4.3.3 Interaction between four characteristics</h3>
      <h4 id="categorical-bias">Categorical bias</h4>
      <p><img style="width:100%" src="/images/Untitled%208.png" alt="Figure 9: In this figure, we look at the interaction bias score for 4 different components. The y-axis represents the missing category that is not in the combination for which the bias is presented. For instance, the first bar is representing the interaction bias score for Religion, Music, Languages, and countries.  "></p>
      <p>Figure 9: In this figure, we look at the interaction bias score for 4 different components. The y-axis represents the missing category that is not in the combination for which the bias is presented. For instance, the first bar is representing the interaction bias score for Religion, Music, Languages, and countries.</p>
      <h4 id="modified-categorical-bias">Modified Categorical bias</h4>
      <p><img style="width:100%" src="/images/Untitled%209.png" alt="Figure 10: In this figure, we look at the weighted interaction bias score for 4 different components. The y-axis represents the missing category that is not in the combination for which the bias is presented. For instance, the first bar is representing the weighted interaction bias score for Religion, Music, Languages, and countries.  "></p>
      <p>Figure 10: In this figure, we look at the weighted interaction bias score for 4 different components. The y-axis represents the missing category that is not in the combination for which the bias is presented. For instance, the first bar is representing the weighted interaction bias score for Religion, Music, Languages, and countries.</p>
      <h3 id="434-interaction-between-5-characteristics">4.3.4 Interaction between 5 characteristics</h3>
      <p>The interaction bias is given by the following numbers</p>
      <table>
      <thead>
      <tr>
      <th>Name of metric</th>
      <th>Bias Score</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>Modified Bias Score</td>
      <td>0.001922595374895377</td>
      </tr>
      <tr>
      <td>Categorical bias score</td>
      <td>0.001922595374895377</td>
      </tr>
      </tbody>
      </table>
      <p>Table 2: The interaction bias score for all five categories together.</p>
      <p><strong>How do we interpret these scores?</strong></p>
      <p>In order to look at the scores, we should have an intuition for how these scores would affect practical situations in real life. Let’s take the worse case, a model that is completely biased would always prefer one group over the other. Let’s say this model prefers Group A over Group B, in this case, the probability of A being selected to be assigned positive attributes would be <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">1-\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span> and the probability of assigning the same attribute would be <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span> for B, where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span> is a small number. The vice-versa would be true for a negative sentiment attribute. For calculation simplicity let <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo>=</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>100</mn></mrow></msup><mo>≈</mo><mn>3</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>44</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\epsilon = e^{-100} \approx 3\times 10 ^{-44}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">100</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">44</span></span></span></span></span></span></span></span></span></span></span></span>. Let’s assume the base rate is close to one. (which is Hence, in this case, the factor <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mi mathvariant="normal">∣</mi><mi>a</mi><mi>t</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>b</mi><mi>u</mi><mi>t</mi><mi>e</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log P(A| attribute)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mord">∣</span><span class="mord mathnormal">a</span><span class="mord mathnormal">tt</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">ib</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mclose">)</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mi mathvariant="normal">∣</mi><mi>a</mi><mi>t</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>b</mi><mi>u</mi><mi>t</mi><mi>e</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log P(B| attribute)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord">∣</span><span class="mord mathnormal">a</span><span class="mord mathnormal">tt</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">ib</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mclose">)</span></span></span></span> would be 0 and -100. Hence the categorical bias score would be 2500. Similarly, for a completely unbiased model, we would assume that the probabilities are the same and equal to 0.5. In this case, one can see the categorical bias score would be 0. One of the ways we can interpret the bias better is just looking at the cases with only 2 classes and seeing how the score function behaves.  The graph below displays the behavior as we can see that the bias score is exponentially increasing with a decrease in the value of epsilon. Hence, one can interpret the score of 0.85 signifying that one group is selected with 0.87 and the other with 0.13. This is a huge difference.</p>
      <p><img style="width:100%" src="/images/Untitled%2010.png" alt="Figure 11: This plot helps us understand the score for only two groups. We assume the base rate to be constant for both elements. "></p>
      <p>Figure 11: This plot helps us understand the score for only two groups. We assume the base rate to be constant for both elements.</p>
      <p>Further, we will conduct the statistical significance test with the null hypothesis that the model is unbiased. In this case, we will assume that an unbiased model will have equal base rates.</p>
      <h3 id="possible-extension-using-next-sentence-prediction">Possible extension using Next Sentence prediction</h3>
      <p>One can use the Next Sentence prediction task to obtain the probabilities and recalculate the scores that will be able to get around the multi-mask challenges that our current method faces.</p>
      <h3 id="section-44-the-limitations-of-our-approach">Section 4.4: The limitations of our approach</h3>
      <p>In this approach, we saw that we weighted adjectives through a sentiment score. This is a way to introduce biases by using another algorithm, as we saw an X number of adjectives didn’t have any sentiment score or a zero sentiment score. We decided to remove those adjectives and only looked at the adjectives with more than zero positive scores. Another point of objection is that this bias score looks for a single adjective and finds the variance failing to capture the way the model treats different people. For instance, it is difficult to identify which groups are particularly marginalized or being discriminated against. Secondly, the masking approach sometimes fails to capture the essence by splitting a particular characteristic like a country name into multiple tokens, however, we only check the first token. For instance, if “Canada” is split into two tokens “Cana” and “da” we only check the probability for the second token.</p>
      <h2 id="appendix-a-the-implicit-association-test">Appendix A. The implicit association test</h2>
      <p>The Implicit Association Test (IAT) is a tool used in psychology to measure subconscious biases in humans. It does this by quantifying the difference in time and accuracy for humans to categorize words as relating to two concepts they find similarities between. For example, people generally respond quicker and more accurately when they are asked to label insects as unpleasant and flowers as pleasant than if they were asked to label these objects in a small amount of time. This is interpreted as indicating that the task is easier due to the subconscious connection between the two concepts. In order to measure subconscious associations of genders with arts and sciences, participants are asked to categorize words as pertaining to (males or the sciences) or (females or the arts) (Nosek et al., 2009). If participants are faster and more accurate in the former setting, it indicates that humans subconsciously associate males with the sciences and females with the arts.</p>
      <p>This same concept can be applied to Natural Language Processing (NLP) models to measure bias in the algorithms. However, one cannot use response time in NLP models to measure this type of bias. Further, the NLP model differs from the brain in the sense the neurons have near and far away connections, and the structure of NLP models has only connections between two layers. Further, many concepts might be stored in associative brain memory that is retrieved and used to find other similar concepts. However, the same is not true for the case of NLP models, hence, in order to find an analog of implicit bias, one must look at the hidden states.</p>
      <h3 id="approach-1-using-dimension-reduction-on-hidden-states-to-find-associations">Approach 1: Using dimension reduction on hidden states to find associations</h3>
      <p>One of the main methods to test for hidden implicit bias is to use hidden states after every encoder in order to find the total activation for different tokens. However, this method usually involves a large number of hidden states for the input of token size 5, given we have 13 hidden outputs in a total of 48,270 elements.</p>
      <p><strong>Preliminary exploration</strong></p>
      <p>In order to find if there are major differences between different classes, we summed the first half of the hidden states and the second half and plotted the difference in order to decide if there is a significant difference between different classes. We found that visualizing the hidden states on an image-like plot did not yield any significant results and displayed few to nondifferences between different characteristics. However, there was a presence of a blue line in nearly every plot that represented the country token in out template case.</p>
      <p><img style="width:100%" src="/images/Untitled%2011.png" alt="Figure 12: The Visualization for the model hidden states for every layer. In this plot, one needs to read from the bottom to the top from the lower y-axis to the higher y-axis representing the progression of layers 1 to 13 of encoders, with every group of 10 rows representing an encoder. One can see that there is not much change in the progression of the hidden states. Representing that this  might be due to the various "></p>
      <p>Figure 12: The Visualization for the model hidden states for every layer. In this plot, one needs to read from the bottom to the top from the lower y-axis to the higher y-axis representing the progression of layers 1 to 13 of encoders, with every group of 10 rows representing an encoder. One can see that there is not much change in the progression of the hidden states. Representing that this  might be due to the various</p>
      <p><strong>Dimension reduction</strong></p>
      <p>In order to find the major differences between the output states we calculated the hidden states and flatten them for the template “A person from {country} is an [adjective]”.  Then we performed various dimension reduction techniques in order to find the outputs that differ the most.</p>
      <p>In this case, we obtained 152 hidden state vectors from the country template. Each vector has 48620 elements and was reduced to a two-element vector through 4 different dimension reduction techniques, those were PCA, TSNE, KMeans, and TruncatedSVD. The last three methods are stochastic methods and we tried 100 iterations of each method with different seeds (from range 0 to 100) and plotted the results. The different iterations of  TSNE, KMeans, and TruncatedSVD didn’t reveal anything visually significant.</p>
      <p><img style="width:100%" src="/images/Untitled%2012.png" alt="Figure 13: In this case, we obtained 152 hidden state vectors from the country template. "></p>
      <p>Figure 13: In this case, we obtained 152 hidden state vectors from the country template.</p>
      <p>We performed a P-test for the null hypothesis that it is no difference between the two sums between the first 6 layers and the last 6 layers and found that the difference was significant</p>
      <p><img style="width:100%" src="/images/Untitled%2013.png" alt="Figure 14: The p-value test between the sum of the first 6 layers and the sum of the last 6 layers. In this case, the sum of the last 6 layers is always smaller than the sum of the first 7 layers. "></p>
      <p>Figure 14: The p-value test between the sum of the first 6 layers and the sum of the last 6 layers. In this case, the sum of the last 6 layers is always smaller than the sum of the first 7 layers.</p>
      <p>This gives us a clue about what to do next. We will plot the histogram of a layer-by-layer sum of activation. Then we will perform a layer-by-layer sum of the hidden states representing the country tokens.</p>
      <p><img style="width:100%" src="/images/Untitled%2014.png" alt="Figure 15: In the given figure one can see the distribution of the sum of hidden states by the layer, we can clearly see that earlier layers have a higher sum than the later ones. The F-value for a one-way f-test is  signifying that not all layers have the same mean. "></p>
      <p>Figure 15: In the given figure one can see the distribution of the sum of hidden states by the layer, we can clearly see that earlier layers have a higher sum than the later ones. The F-value for a one-way f-test is <code>85375.67345903238</code> signifying that not all layers have the same mean.</p>
      <p><img style="width:100%" src="/images/Untitled%2015.png" alt="Figure 16: In the given figure one can see the distribution of the sum of hidden states by the layer, we can clearly see that earlier layers have a higher sum than the later ones. The F-value for a one-way f-test is  signifying that not all layers have the same mean. "></p>
      <p>Figure 16: In the given figure one can see the distribution of the sum of hidden states by the layer, we can clearly see that earlier layers have a higher sum than the later ones. The F-value for a one-way f-test is <code>77135.0478500179</code> signifying that not all layers have the same mean.</p>
      <p>In figure 16, we can see that the activation from layer 11 to layer 12 varies drastically, signaling in the last layer there is a big shift and the middle layers from 4 to 10 there is no major changes between the layers.</p>
      <h3 id="approach-2-using-the-analogy-of-neurotransmitters">Approach 2: Using the analogy of neurotransmitters</h3>
      <p>Neurotransmitters are chemicals that are released by neurons in the brain to transmit signals to other neurons. These chemicals play a crucial role in the functioning of the brain, as they allow neurons to communicate with each other and form the basis of neural networks.</p>
      <p>In the same way, some hidden states in NLP models can be thought of as the &quot;neurotransmitters&quot; that allow the different layers of the model to communicate with each other and form a network. Just as neurotransmitters can carry information about the state of the brain and its biases, hidden states in NLP models can carry information about the biases present in the model. By analyzing the hidden states of an NLP model, one can identify and measure the biases present in the model.</p>
      <p>However, it is important to note that this analogy is not perfect, as the brain and NLP models are fundamentally different in their structure and function. The brain is a complex, highly interconnected network of neurons, while NLP models are more simplified and have only connections between two layers. Additionally, neurotransmitters in the brain are involved in a wide range of processes, while hidden states in NLP models are only one aspect of the model's functioning. Furthermore, the brain is capable of learning and adapting, while NLP models are typically trained on a fixed dataset and do not have the ability to adapt in the same way. Therefore, while the analogy of hidden states as &quot;neurotransmitters&quot; can be useful, it should not be taken too literally.</p>
      <p>We hypothesize the presence of promoters and demoters circuits (or a network of neurons) that play a key role in promoting or inhibiting bias in neural networks.</p>
      <h2 id="appendix-b-framework-for-evaluating-continuous-bias-evaluation">Appendix B: Framework for evaluating continuous bias evaluation</h2>
      <p>Currently, the main steps to identify and mitigate bias while deploying models are described below:</p>
      <ol>
      <li>Define the purpose and intended use of the NLP model.</li>
      <li>Identify potential sources of bias in the data used to train the model.</li>
      <li>Implement strategies to mitigate bias in the data, such as oversampling underrepresented groups or using data augmentation techniques.</li>
      <li>Test the model on a diverse set of inputs to ensure it performs well on a wide range of data.</li>
      <li>Evaluate the model's performance using metrics designed to measure bias, such as the Categorical Bias score or the Implicit Bias Association test.</li>
      <li>Conduct a thorough error analysis to identify any potential biases in the model's output.</li>
      <li>Iterate on the model and repeat the evaluation process until satisfactory performance is achieved.</li>
      <li>Before deploying the model, conduct additional testing to ensure it performs well in the intended environment and use cases.</li>
      <li>Continuously monitor the model's performance post-deployment and make updates as needed to address any biases that may arise.</li>
      </ol>
      <p>It is difficult to argue that the above framework for identifying and evaluating biases in NLP models is ineffective, as it provides a thorough and comprehensive approach to addressing potential biases in the data and ensuring the model performs well on a diverse range of inputs. However, there are a few potential criticisms of this framework that could be raised:</p>
      <ol>
      <li>One potential criticism is that the framework does not explicitly address the potential for bias in the model's architecture or training process. In order to truly mitigate bias in an NLP model, it is important to not only consider the data used to train the model, but also the design of the model itself and the algorithms used to train it.</li>
      <li>Another potential criticism is that the framework does not provide clear guidelines or benchmarks for determining when a model is performing well enough to be considered bias-free. The use of metrics like the Categorical Bias score and the Implicit Bias Association test can be helpful in identifying biases, but these metrics do not provide a definitive measure of a model's performance.</li>
      <li>Finally, the framework does not consider the potential for bias to arise post-deployment, even with ongoing monitoring and updates. Because real-world data and use cases can change over time, there is always a risk that biases may emerge in a model's performance after it has been deployed. This potential for bias post-deployment highlights the need for continued vigilance and regular testing and evaluation of NLP models.</li>
      <li>The above steps are very difficult to implement as it does not have clear guidelines for each step and no particular hard metrics.</li>
      <li>This framework evaluates bias as an afterthought after training the model, hence, this is problematic because it deprioritized the bias identification. Many companies could potentially deprioritize as this does not give gains.</li>
      </ol>
      <p>Hence, in order to change the system one needs to change this behavior through 3 different methods that deal with incentivizing the industry.</p>
      <ol>
      <li>Defining a loss function to incorporate elements of biases through an epoch-based approach.</li>
      <li>A library for using the new cultural bias metrics</li>
      <li>Creating an NLP model bias evaluation organization that could certify algorithms as bias tested. Analogous to LEED certification for architecture.</li>
      </ol>
      <h2 id="appendix-c-gathering-data">Appendix C: Gathering Data:</h2>
      <p>In this paper, we utilized multiple data sources that included some self-constructed, and others scraped from different websites.</p>
      <ol>
      <li>Country dataset - <a href="https://pypi.org/project/country-list/">https://pypi.org/project/country-list/</a></li>
      <li>Dishes - <a href="https://en.wikipedia.org/wiki/National_dish">https://en.wikipedia.org/wiki/National_dish</a></li>
      <li>Languages - ISO</li>
      <li>Religions - <a href="http://google.com">google.com</a></li>
      <li>Music genres - <a href="http://google.com">google.com</a></li>
      </ol>
      <p>The list of adjectives was downloaded from <a href="https://gist.github.com/hugsy/8910dc78d208e40de42deb29e62df913">https://gist.github.com/hugsy/8910dc78d208e40de42deb29e62df913</a></p>
      <p>The data collected was processed and filtered to suit our needs and can be seen freely on <a href="https://github.com/beyondinfinities/Capstone_BERT">https://github.com/beyondinfinities/Capstone_BERT</a></p>
      <h2 id="appendix-d-failed-graphs">Appendix D: Failed graphs</h2>
      <p>Multiple graphs for dimensional reduction technique using different random states.</p>
      <p><img style="width:100%" src="/images/Untitled%2016.png" alt="Untitled"></p>
      <p><img style="width:100%" src="/images/Untitled%2017.png" alt="Untitled"></p>
      

  </d-article>

  <d-appendix>
    <h3>Acknowledgments</h3>
    <p>
      I am grateful to my advisor Prof. Watson for his undying dupport, direction and actionable feedback. I am also grateful to my friends and family for their support and encouragement.
      
    </p>

    <h3>Author Contributions</h3>
    <p>
    All the figures and tables can be reproduced via the code that is available in this <a href="https://github.com/BeyondInfinities/Capstone_BERT">github repository</a>. The code for the paper can be found in <a href="https://github.com/BeyondInfinities/Capstone_Deliverable">this repository.</a>
    </p>
    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <script src="/public/citations.bib" type="text/bibliography">
    @article{cite1,
      author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
      title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
      doi = {10.48550/arXiv.1810.04805},
      url = {https://arxiv.org/abs/1810.04805},
      urldate = {2023-02-24},
      year = {2018},
      journal = {arXiv.org}
    } 
  </script>
  <script src="/citations.bib" type="text/bibliography">
    @article{cite1,
      author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
      title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
      doi = {10.48550/arXiv.1810.04805},
      url = {https://arxiv.org/abs/1810.04805},
      urldate = {2023-02-24},
      year = {2018},
      journal = {arXiv.org}
    } 
  </script>
    

  <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
  <d-bibliography src="citations.bib"></d-bibliography>

<script type="text/javascript" src="index.bundle.js"></script></body>
